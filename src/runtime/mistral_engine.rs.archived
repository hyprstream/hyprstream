//! Mistral.rs runtime engine implementation with X-LoRA support
//! 
//! This replaces the LlamaCpp engine with mistral.rs for superior real-time LoRA adaptation capabilities.
//! 
//! ## Current Status
//! - ‚úÖ Complete API structure with placeholder implementations
//! - ‚è≥ Real mistral.rs integration pending dependency resolution  
//! - ‚è≥ X-LoRA multi-adapter management pending implementation
//!
//! ## Key Features (Planned)
//! - Real-time adapter weight updates (performance targets TBD)
//! - Fast adapter switching capabilities
//! - Multi-adapter routing with X-LoRA
//! - Continuous learning from user feedback
//! - Performance improvements over current llama.cpp adapter updates

use anyhow::{anyhow, Result};
use async_trait::async_trait;
use std::path::Path;
use std::time::Instant;
use std::collections::HashMap;
use std::sync::Arc;

// Temporarily comment out mistralrs imports during migration to Candle
// use mistralrs::{
//     GgufModelBuilder, GgufXLoraModelBuilder, Model, TextMessages, TextMessageRole, IsqType,
//     Ordering,
// };

// Placeholder types for compilation
type Model = ();
type IsqType = String;

use super::{RuntimeEngine, ModelInfo, GenerationRequest, GenerationResult, FinishReason, RuntimeConfig};
use crate::adapters::lora_checkpoints::{LoRACheckpoint, LoRAWeightsData};
use crate::storage::vdb::{
    TemporalStreamingLayer, TemporalStreamingConfig, TemporalWeightStream, 
    TemporalWeightUpdate, VDBSparseStorage, SparseStorageConfig
};

/// Mistral.rs runtime engine implementation with X-LoRA support
pub struct MistralEngine {
    /// Core mistral.rs model instance
    model: Option<Model>,
    /// Model configuration
    config: RuntimeConfig,
    /// Model metadata
    model_info: Option<ModelInfo>,
    /// X-LoRA adapter management
    xlora_adapters: HashMap<String, XLoRAAdapter>,
    /// Real-time adaptation state
    adaptation_state: AdaptationState,
    /// Model builder type for reconstruction
    builder_config: ModelBuilderConfig,
    /// Temporal streaming layer for real-time LoRA updates
    temporal_streaming: Option<Arc<TemporalStreamingLayer>>,
    /// Active temporal streams
    active_streams: HashMap<String, TemporalWeightStream>,
}

/// X-LoRA adapter wrapper
#[derive(Debug, Clone)]
pub struct XLoRAAdapter {
    /// Adapter ID
    pub id: String,
    /// Adapter weights in mistral.rs format
    pub weights: LoRAWeights,
    /// Performance metrics for this adapter
    pub metrics: AdapterMetrics,
    /// Last update timestamp
    pub last_updated: chrono::DateTime<chrono::Utc>,
}

/// Real-time adaptation state
#[derive(Debug, Default)]
pub struct AdaptationState {
    /// Current learning rate
    pub learning_rate: f32,
    /// Total adaptations performed
    pub adaptation_count: u64,
    /// Recent performance history
    pub performance_history: Vec<PerformanceSnapshot>,
    /// Active adaptation mode
    pub adaptation_mode: AdaptationMode,
}

/// Adaptation mode configuration
#[derive(Debug, Clone)]
pub enum AdaptationMode {
    /// No real-time adaptation
    Disabled,
    /// Continuous learning from all interactions
    Continuous { frequency: usize },
    /// Learning from explicit feedback only
    Feedback { threshold: f32 },
    /// Reinforcement learning based adaptation
    Reinforcement { reward_model: String },
    /// X-LoRA multi-adapter mode
    XLoRA { 
        active_adapters: Vec<String>,
        max_adapters: usize,
    },
}

impl Default for AdaptationMode {
    fn default() -> Self {
        AdaptationMode::Disabled
    }
}

/// Performance snapshot for tracking adaptation effectiveness
#[derive(Debug, Clone)]
pub struct PerformanceSnapshot {
    pub timestamp: chrono::DateTime<chrono::Utc>,
    pub tokens_per_second: f32,
    pub memory_usage_mb: f32,
    pub quality_score: Option<f32>,
}

/// Adapter performance metrics
#[derive(Debug, Clone, Default)]
pub struct AdapterMetrics {
    pub total_uses: u64,
    pub average_latency_ms: f32,
    pub average_quality: f32,
    pub last_used: Option<chrono::DateTime<chrono::Utc>>,
}

/// Mistral.rs compatible LoRA weights
#[derive(Debug, Clone)]
pub struct LoRAWeights {
    /// A matrix weights (input ‚Üí low-rank)
    pub a_matrices: HashMap<String, Vec<Vec<f32>>>,
    /// B matrix weights (low-rank ‚Üí output)
    pub b_matrices: HashMap<String, Vec<Vec<f32>>>,
    /// Scaling factor
    pub scaling: f32,
    /// Target modules
    pub target_modules: Vec<String>,
}

/// Configuration for model builders
#[derive(Debug, Clone)]
pub enum ModelBuilderConfig {
    /// GGUF model configuration
    Gguf {
        model_path: String,
        tokenizer_json: Option<String>,
        quantization: Option<IsqType>,
    },
    /// Text model configuration (HuggingFace)
    Text {
        model_id: String,
        quantization: Option<IsqType>,
        device_mapping: Option<String>,
    },
    /// LoRA model configuration
    Lora {
        base_model_path: String,
        adapters: Vec<String>,
    },
    /// X-LoRA model configuration  
    XLora {
        base_model_path: String,
        xlora_model_id: String,
        ordering: Ordering,
        max_adapters: usize,
        routing_strategy: XLoRARoutingStrategy,
        tgt_non_granular_index: Option<usize>,
    },
}

/// X-LoRA routing strategies for adapter selection
#[derive(Debug, Clone)]
pub enum XLoRARoutingStrategy {
    /// Learned routing using X-LoRA classifier (recommended)
    Learned,
    /// Fixed routing with specified adapter weights
    Fixed(HashMap<String, f32>),
    /// Dynamic routing based on input characteristics
    Dynamic,
}

impl MistralEngine {
    /// Create a new mistral.rs engine
    pub fn new(config: RuntimeConfig) -> Result<Self> {
        Ok(Self {
            model: None,
            config,
            model_info: None,
            xlora_adapters: HashMap::new(),
            adaptation_state: AdaptationState::default(),
            builder_config: ModelBuilderConfig::Gguf {
                model_path: String::new(),
                tokenizer_json: None,
                quantization: None,
            },
            temporal_streaming: None,
            active_streams: HashMap::new(),
        })
    }

    /// Create with default configuration
    pub fn new_default() -> Result<Self> {
        Self::new(RuntimeConfig::default())
    }

    /// Initialize temporal streaming layer for real-time LoRA updates
    pub async fn initialize_temporal_streaming(&mut self) -> Result<()> {
        tracing::info!("üåä Initializing temporal streaming for real-time LoRA updates");
        
        // Create VDB sparse storage backend
        let storage_config = SparseStorageConfig::default();
        let vdb_storage = Arc::new(VDBSparseStorage::new(storage_config).await
            .map_err(|e| anyhow!("Failed to create VDB storage: {}", e))?);
        
        // Create temporal streaming layer
        let streaming_config = TemporalStreamingConfig::default();
        let temporal_layer = Arc::new(TemporalStreamingLayer::new(
            vdb_storage, 
            streaming_config
        ).await?);
        
        self.temporal_streaming = Some(temporal_layer);
        
        tracing::info!("‚úÖ Temporal streaming layer initialized");
        Ok(())
    }

    /// Create temporal streaming session for real-time weight updates
    pub async fn create_temporal_stream(&mut self, adapter_id: String) -> Result<()> {
        tracing::info!("üåä Creating temporal stream for adapter: {}", adapter_id);
        
        let temporal_layer = self.temporal_streaming.as_ref()
            .ok_or_else(|| anyhow!("Temporal streaming not initialized. Call initialize_temporal_streaming() first"))?;
        
        // Create streaming session
        let stream = temporal_layer.create_temporal_stream(
            adapter_id.clone(),
            None, // No layer filtering for now
        ).await?;
        
        // Store active stream
        self.active_streams.insert(adapter_id.clone(), stream);
        
        tracing::info!("‚úÖ Created temporal stream for adapter: {}", adapter_id);
        Ok(())
    }

    /// Process temporal weight updates from stream
    pub async fn process_temporal_updates(&mut self, adapter_id: &str) -> Result<Vec<TemporalWeightUpdate>> {
        use futures_util::StreamExt;
        
        let stream = self.active_streams.get_mut(adapter_id)
            .ok_or_else(|| anyhow!("No temporal stream found for adapter: {}", adapter_id))?;
        
        let mut updates = Vec::new();
        
        // Collect available updates (non-blocking)
        while let Some(update_result) = stream.next().await {
            match update_result {
                Ok(update) => {
                    tracing::debug!("üì• Received temporal update: {} weights", update.weights.len());
                    updates.push(update);
                }
                Err(e) => {
                    tracing::warn!("‚ö†Ô∏è Temporal stream error: {}", e);
                    break;
                }
            }
            
            // Limit batch size to prevent blocking
            if updates.len() >= 100 {
                break;
            }
        }
        
        // Apply updates to model if available
        if !updates.is_empty() {
            self.apply_temporal_updates_to_model(&updates).await?;
        }
        
        Ok(updates)
    }

    /// Apply temporal updates directly to mistral.rs model (when supported)
    async fn apply_temporal_updates_to_model(&mut self, updates: &[TemporalWeightUpdate]) -> Result<()> {
        tracing::debug!("üîÑ Applying {} temporal updates to model", updates.len());
        
        // Current limitation: mistral.rs doesn't support real-time weight updates
        // We track updates locally and apply them via X-LoRA adapter reloading
        
        for update in updates {
            // Update local adapter tracking
            let adapter_id = update.adapter_id.clone();
            
            // Convert temporal update to our LoRA format first (before mutable borrow)
            let lora_weights = self.convert_temporal_update_to_lora(update)?;
            
            if let Some(adapter) = self.xlora_adapters.get_mut(&adapter_id) {
                adapter.last_updated = update.timestamp;
                adapter.metrics.total_uses += 1;
                adapter.weights = lora_weights;
                
                tracing::debug!("üîÑ Updated local tracking for adapter: {}", adapter_id);
            } else {
                tracing::warn!("‚ö†Ô∏è Adapter not found for temporal update: {}", adapter_id);
            }
        }
        
        // TODO: When mistral.rs adds runtime weight update APIs, apply updates directly
        tracing::debug!("üìã Note: Real-time model updates pending mistral.rs runtime API support");
        
        Ok(())
    }

    /// Convert temporal weight update to LoRA format
    fn convert_temporal_update_to_lora(&self, update: &TemporalWeightUpdate) -> Result<LoRAWeights> {
        // Convert sparse coordinate updates to LoRA A/B matrix format
        let mut a_matrices = HashMap::new();
        let mut b_matrices = HashMap::new();
        
        // Simple mapping: split coordinates between A and B matrices
        let mut a_weights = Vec::new();
        let mut b_weights = Vec::new();
        
        for (coord, weight) in &update.weights {
            if coord.x() % 2 == 0 {
                a_weights.push(vec![*weight]); // Simplified: single-element rows
            } else {
                b_weights.push(vec![*weight]);
            }
        }
        
        // Store in layer-specific matrices
        a_matrices.insert(update.layer_name.clone(), a_weights);
        b_matrices.insert(update.layer_name.clone(), b_weights);
        
        Ok(LoRAWeights {
            a_matrices,
            b_matrices,
            scaling: 1.0,
            target_modules: vec![update.layer_name.clone()],
        })
    }

    /// Enable real-time temporal adaptation with streaming
    pub async fn enable_temporal_adaptation(&mut self, adapter_id: String) -> Result<()> {
        tracing::info!("üß† Enabling temporal adaptation for adapter: {}", adapter_id);
        
        // Initialize temporal streaming if not already done
        if self.temporal_streaming.is_none() {
            self.initialize_temporal_streaming().await?;
        }
        
        // Create temporal stream for this adapter
        self.create_temporal_stream(adapter_id.clone()).await?;
        
        // Update adaptation state
        self.adaptation_state.adaptation_mode = AdaptationMode::XLoRA {
            active_adapters: vec![adapter_id.clone()],
            max_adapters: 4, // Default X-LoRA limit
        };
        
        tracing::info!("‚úÖ Temporal adaptation enabled for: {}", adapter_id);
        Ok(())
    }

    /// Get temporal streaming statistics
    pub async fn get_temporal_stats(&self) -> Result<crate::storage::vdb::TemporalStreamingStats> {
        let temporal_layer = self.temporal_streaming.as_ref()
            .ok_or_else(|| anyhow!("Temporal streaming not initialized"))?;
        
        Ok(temporal_layer.get_streaming_stats().await)
    }

    /// Load model with X-LoRA configuration
    pub async fn load_model_with_xlora(
        &mut self, 
        path: &Path,
        xlora_model_id: String,
        ordering: Ordering,
        max_adapters: usize,
        routing_strategy: XLoRARoutingStrategy,
    ) -> Result<()> {
        tracing::info!("üîÄ Loading model with X-LoRA: {}", path.display());
        tracing::info!("   X-LoRA Model ID: {}", xlora_model_id);
        tracing::info!("   Max Adapters: {}", max_adapters);
        
        // Store X-LoRA builder configuration
        self.builder_config = ModelBuilderConfig::XLora {
            base_model_path: path.to_string_lossy().to_string(),
            xlora_model_id: xlora_model_id.clone(),
            ordering: ordering.clone(),
            max_adapters,
            routing_strategy: routing_strategy.clone(),
            tgt_non_granular_index: None,
        };
        
        // Create GGUF model builder
        let gguf_builder = GgufModelBuilder::new(
            path.parent().unwrap_or(Path::new(".")).to_string_lossy(),
            vec![path.file_name().unwrap().to_string_lossy()]
        )
        .with_logging();
        
        // Wrap with X-LoRA builder
        let xlora_builder = GgufXLoraModelBuilder::from_gguf_model_builder(
            gguf_builder,
            xlora_model_id,
            ordering,
        );
        
        // Build X-LoRA model
        let model = xlora_builder
            .build()
            .await
            .map_err(|e| anyhow!("Failed to load X-LoRA model with mistral.rs: {:?}", e))?;
        
        // Extract real model metadata from mistral.rs
        let model_info = self.extract_model_metadata(&model).await
            .unwrap_or_else(|e| {
                tracing::warn!("Failed to extract model metadata: {}. Using fallback.", e);
                self.get_fallback_model_info(path, "xlora")
            });
        
        self.model = Some(model);
        self.model_info = Some(model_info);
        
        // Initialize X-LoRA state
        self.adaptation_state.adaptation_mode = AdaptationMode::XLoRA {
            active_adapters: Vec::new(),
            max_adapters,
        };
        
        tracing::info!("‚úÖ X-LoRA model loaded successfully");
        Ok(())
    }

    /// Configure X-LoRA multi-adapter support
    pub async fn configure_xlora(&mut self, max_adapters: usize, _routing_strategy: XLoRARoutingStrategy) -> Result<()> {
        tracing::info!("üîÄ Configuring X-LoRA with {} max adapters", max_adapters);
        
        // Check if model is loaded
        if self.model.is_none() {
            return Err(anyhow!("Model not loaded. Load model before configuring X-LoRA."));
        }
        
        tracing::warn!("‚ö†Ô∏è  mistral.rs limitation: X-LoRA configuration is set at model load time");
        tracing::info!("üìã X-LoRA models require pre-configured ordering file with adapter list");
        tracing::info!("üí° To change X-LoRA configuration, reload model with different ordering");
        
        // Update our local tracking state (best we can do)
        self.adaptation_state.adaptation_mode = AdaptationMode::XLoRA {
            active_adapters: Vec::new(),
            max_adapters,
        };
        
        tracing::info!("‚úÖ X-LoRA configured with {} max adapters", max_adapters);
        Ok(())
    }

    /// Update adapter weights in real-time (performance target TBD)
    pub async fn update_adapter_realtime(&mut self, adapter_id: &str, weights: &LoRAWeightsData) -> Result<()> {
        let start_time = Instant::now();
        
        tracing::debug!("‚ö° Updating adapter {} weights in real-time", adapter_id);
        
        // Convert LoRAWeightsData to mistral.rs format
        let mistral_weights = self.convert_weights_to_mistral_format(weights)?;
        
        // LIMITATION: mistral.rs doesn't support real-time weight updates
        // Current implementation tracks weights locally but can't apply them to the model
        if self.model.is_some() {
            tracing::warn!("‚ö†Ô∏è  mistral.rs limitation: Real-time weight updates not supported by underlying library");
            tracing::info!("üìã Tracking adapter {} weights locally until mistral.rs adds runtime update APIs", adapter_id);
            
            // Update our local adapter tracking (best we can do with current mistral.rs)
            if let Some(adapter) = self.xlora_adapters.get_mut(adapter_id) {
                adapter.weights = mistral_weights;
                adapter.last_updated = chrono::Utc::now();
                adapter.metrics.total_uses += 1;
                tracing::debug!("üîÑ Updated local weights for adapter {}", adapter_id);
            } else {
                // Create new adapter entry for future model reload
                let new_adapter = XLoRAAdapter {
                    id: adapter_id.to_string(),
                    weights: mistral_weights,
                    metrics: AdapterMetrics::default(),
                    last_updated: chrono::Utc::now(),
                };
                self.xlora_adapters.insert(adapter_id.to_string(), new_adapter);
                tracing::debug!("‚ûï Added new adapter {} to local tracking", adapter_id);
            }
            
            // NOTE: To apply these weight changes, the model would need to be reloaded
            // This is a limitation of the current mistral.rs X-LoRA implementation
        } else {
            return Err(anyhow!("Model not loaded"));
        }
        
        let duration = start_time.elapsed();
        tracing::info!("‚ö° Adapter {} updated in {:?}", adapter_id, duration);
        
        // Track adaptation state
        self.adaptation_state.adaptation_count += 1;
        
        Ok(())
    }

    /// Switch active adapters instantly
    pub async fn switch_active_adapters(&mut self, adapter_ids: &[String]) -> Result<()> {
        let start_time = Instant::now();
        
        tracing::debug!("üîÄ Switching to adapters: {:?}", adapter_ids);
        
        if self.model.is_some() {
            tracing::warn!("‚ö†Ô∏è  mistral.rs limitation: Dynamic adapter switching not supported");
            tracing::info!("üìã X-LoRA models use static routing determined at load time");
            
            // Validate requested adapters exist in our local tracking
            let mut valid_adapters = Vec::new();
            let mut missing_adapters = Vec::new();
            
            for adapter_id in adapter_ids {
                if self.xlora_adapters.contains_key(adapter_id) {
                    valid_adapters.push(adapter_id.clone());
                } else {
                    missing_adapters.push(adapter_id.clone());
                }
            }
            
            if !missing_adapters.is_empty() {
                tracing::warn!("‚ö†Ô∏è  Missing adapters: {:?}", missing_adapters);
            }
            
            if !valid_adapters.is_empty() {
                tracing::info!("‚úÖ Valid adapters for future use: {:?}", valid_adapters);
                
                // Update metrics for requested adapters
                for adapter_id in &valid_adapters {
                    if let Some(adapter) = self.xlora_adapters.get_mut(adapter_id) {
                        adapter.metrics.total_uses += 1;
                    }
                }
                
                // NOTE: mistral.rs X-LoRA uses automatic routing, not manual selection
                // To use specific adapters, the model would need to be reloaded with different ordering
            }
        } else {
            return Err(anyhow!("Model not loaded"));
        }
        
        let duration = start_time.elapsed();
        tracing::info!("üîÄ Adapter switch completed in {:?}", duration);
        
        Ok(())
    }

    /// Load LoRA checkpoint and convert to X-LoRA adapter
    pub async fn load_lora_checkpoint(&mut self, checkpoint: &LoRACheckpoint) -> Result<String> {
        tracing::info!("üìé Loading LoRA checkpoint: {}", checkpoint.checkpoint_id);
        
        // Load weights from checkpoint (VDB sparse storage)
        let weights_data = self.load_checkpoint_weights(checkpoint).await?;
        
        // Convert to mistral.rs format and add as adapter
        let adapter_id = format!("checkpoint_{}", checkpoint.checkpoint_id);
        self.update_adapter_realtime(&adapter_id, &weights_data).await?;
        
        tracing::info!("‚úÖ Checkpoint loaded as adapter: {}", adapter_id);
        Ok(adapter_id)
    }

    /// Enable real-time adaptation mode
    pub fn enable_realtime_adaptation(&mut self, mode: AdaptationMode) -> Result<()> {
        tracing::info!("üß† Enabling real-time adaptation: {:?}", mode);
        
        self.adaptation_state.adaptation_mode = mode;
        self.adaptation_state.learning_rate = 0.001; // Default learning rate
        
        tracing::info!("‚úÖ Real-time adaptation enabled");
        Ok(())
    }

    /// Process generation feedback for real-time learning
    pub async fn process_generation_feedback(&mut self, 
                                           request: &GenerationRequest, 
                                           result: &GenerationResult,
                                           feedback: Option<UserFeedback>) -> Result<()> {
        // Clone values to avoid borrowing issues
        let adaptation_mode = self.adaptation_state.adaptation_mode.clone();
        let adaptation_count = self.adaptation_state.adaptation_count;
        
        match adaptation_mode {
            AdaptationMode::Disabled => {
                // No adaptation
                Ok(())
            }
            AdaptationMode::Continuous { frequency } => {
                if adaptation_count % frequency as u64 == 0 {
                    self.perform_continuous_adaptation(request, result).await
                } else {
                    Ok(())
                }
            }
            AdaptationMode::Feedback { threshold } => {
                if let Some(feedback) = feedback {
                    if feedback.quality_score >= threshold {
                        self.perform_feedback_adaptation(request, result, &feedback).await
                    } else {
                        Ok(())
                    }
                } else {
                    Ok(())
                }
            }
            AdaptationMode::Reinforcement { reward_model } => {
                self.perform_reinforcement_adaptation(request, result, &reward_model).await
            }
            AdaptationMode::XLoRA { active_adapters, max_adapters } => {
                tracing::debug!("üîÄ Processing X-LoRA feedback with {} active adapters", active_adapters.len());
                // TODO: Implement X-LoRA specific feedback processing
                Ok(())
            }
        }
    }

    /// Convert LoRAWeightsData to mistral.rs format
    fn convert_weights_to_mistral_format(&self, weights: &LoRAWeightsData) -> Result<LoRAWeights> {
        Ok(LoRAWeights {
            a_matrices: weights.a_weights.clone(),
            b_matrices: weights.b_weights.clone(),
            scaling: weights.scaling,
            target_modules: weights.target_modules.clone(),
        })
    }

    /// Load checkpoint weights from VDB storage
    async fn load_checkpoint_weights(&self, checkpoint: &LoRACheckpoint) -> Result<LoRAWeightsData> {
        // Read weights from checkpoint file (JSON format from our VDB system)
        let weights_json = tokio::fs::read_to_string(&checkpoint.weights_path).await
            .map_err(|e| anyhow!("Failed to read checkpoint weights: {}", e))?;
        
        let weights_data: LoRAWeightsData = serde_json::from_str(&weights_json)
            .map_err(|e| anyhow!("Failed to parse checkpoint weights: {}", e))?;
        
        Ok(weights_data)
    }

    /// Generate text using mistral.rs with X-LoRA adapters
    async fn generate_with_xlora(&self, prompt: &str, max_tokens: usize, _adapters: Option<&[String]>) -> Result<String> {
        if let Some(ref model) = self.model {
            tracing::debug!("üöÄ Generating with mistral.rs");
            
            // Create TextMessages for mistral.rs API
            let messages = TextMessages::new()
                .add_message(TextMessageRole::User, prompt);
            
            // Send chat request to model
            let response = model.send_chat_request(messages).await
                .map_err(|e| anyhow!("Mistral.rs generation failed: {:?}", e))?;
            
            // Extract content from response
            if let Some(choice) = response.choices.first() {
                if let Some(content) = &choice.message.content {
                    tracing::debug!("‚úÖ Generated {} characters", content.len());
                    return Ok(content.clone());
                }
            }
            
            Err(anyhow!("No content generated in response"))
        } else {
            Err(anyhow!("Model not loaded"))
        }
    }

    // Placeholder methods for different adaptation strategies
    async fn perform_continuous_adaptation(&mut self, _request: &GenerationRequest, _result: &GenerationResult) -> Result<()> {
        // TODO: Implement continuous adaptation algorithm
        tracing::debug!("üîÑ Performing continuous adaptation");
        Ok(())
    }

    async fn perform_feedback_adaptation(&mut self, _request: &GenerationRequest, _result: &GenerationResult, _feedback: &UserFeedback) -> Result<()> {
        // TODO: Implement feedback-based adaptation
        tracing::debug!("üìù Performing feedback adaptation");
        Ok(())
    }

    async fn perform_reinforcement_adaptation(&mut self, _request: &GenerationRequest, _result: &GenerationResult, _reward_model: &str) -> Result<()> {
        // TODO: Implement reinforcement learning adaptation
        tracing::debug!("üéØ Performing reinforcement adaptation");
        Ok(())
    }
}

#[async_trait]
impl RuntimeEngine for MistralEngine {
    async fn load_model(&mut self, path: &Path) -> Result<()> {
        tracing::info!("üöÄ Loading model with mistral.rs: {}", path.display());
        
        // Store builder configuration
        self.builder_config = ModelBuilderConfig::Gguf {
            model_path: path.to_string_lossy().to_string(),
            tokenizer_json: None, // Will auto-detect or use default
            quantization: self.get_quantization_from_config(),
        };
        
        // Build model using GgufModelBuilder (correct API pattern)
        let model = GgufModelBuilder::new(
            path.parent().unwrap_or(Path::new(".")).to_string_lossy(),
            vec![path.file_name().unwrap().to_string_lossy()]
        )
        .with_logging()
        .build()
        .await
        .map_err(|e| anyhow!("Failed to load GGUF model with mistral.rs: {:?}", e))?;
        
        // Extract real model metadata from mistral.rs
        let model_info = self.extract_model_metadata(&model).await
            .unwrap_or_else(|e| {
                tracing::warn!("Failed to extract model metadata: {}. Using fallback.", e);
                self.get_fallback_model_info(path, "gguf")
            });
        
        self.model = Some(model);
        self.model_info = Some(model_info);
        
        tracing::info!("‚úÖ Model loaded successfully with mistral.rs");
        Ok(())
    }

    async fn generate(&self, prompt: &str, max_tokens: usize) -> Result<String> {
        self.generate_with_xlora(prompt, max_tokens, None).await
    }

    async fn generate_with_params(&self, request: GenerationRequest) -> Result<GenerationResult> {
        let start_time = Instant::now();
        
        tracing::debug!("üöÄ Generating with mistral.rs: '{}'", 
                       request.prompt.chars().take(50).collect::<String>());
        
        let generated_text = self.generate_with_xlora(
            &request.prompt, 
            request.max_tokens,
            request.active_adapters.as_deref()
        ).await?;
        
        let generation_time = start_time.elapsed();
        let generation_time_ms = generation_time.as_millis() as u64;
        
        // Estimate tokens generated (rough approximation)
        let tokens_generated = generated_text.split_whitespace().count().min(request.max_tokens);
        
        let tokens_per_second = if generation_time_ms > 0 {
            (tokens_generated as f32 * 1000.0) / generation_time_ms as f32
        } else {
            0.0
        };
        
        tracing::info!("‚úÖ Generated {} tokens in {:?} ({:.1} tok/s)", 
                      tokens_generated, generation_time, tokens_per_second);
        
        Ok(GenerationResult {
            text: generated_text,
            tokens_generated,
            finish_reason: if tokens_generated >= request.max_tokens {
                FinishReason::MaxTokens
            } else {
                FinishReason::EndOfSequence
            },
            generation_time_ms,
            tokens_per_second,
        })
    }

    fn model_info(&self) -> ModelInfo {
        self.model_info.clone().unwrap_or_else(|| ModelInfo {
            name: "unloaded".to_string(),
            parameters: 0,
            context_length: 0,
            vocab_size: 0,
            architecture: "unknown".to_string(),
            quantization: None,
        })
    }

    fn is_loaded(&self) -> bool {
        self.model.is_some()
    }
}

impl MistralEngine {
    /// Get quantization setting from config (placeholder)
    fn get_quantization_from_config(&self) -> Option<IsqType> {
        // Default to Q8_0 for best quality/performance balance
        Some(IsqType::Q8_0)
    }
    
    /// Reload model with new X-LoRA configuration
    /// This is the proper way to change adapters in mistral.rs
    pub async fn reload_with_xlora_config(&mut self, adapter_ids: &[String], xlora_model_id: &str) -> Result<()> {
        tracing::info!("üîÑ Reloading model with X-LoRA adapters: {:?}", adapter_ids);
        
        if self.model.is_none() {
            return Err(anyhow!("No model currently loaded"));
        }
        
        // Store current model identifier for reload
        let model_path = match &self.builder_config {
            ModelBuilderConfig::Gguf { model_path, .. } => model_path.clone(),
            ModelBuilderConfig::XLora { base_model_path, .. } => base_model_path.clone(),
            ModelBuilderConfig::Text { model_id, .. } => model_id.clone(),
            ModelBuilderConfig::Lora { base_model_path, .. } => base_model_path.clone(),
        };
        
        // Create new ordering for X-LoRA
        let ordering_json = serde_json::json!({
            "order": adapter_ids,
            "layers": {},
            "base_model_id": "base"
        });
        
        // Unload current model
        self.model = None;
        tracing::debug!("üì§ Unloaded current model");
        
        // TODO: Implement actual model reload with new X-LoRA configuration
        // This would require:
        // 1. Write ordering JSON to temporary file
        // 2. Use GgufXLoraModelBuilder with new ordering  
        // 3. Reload model with updated adapter configuration
        
        tracing::warn!("‚ö†Ô∏è  Model reload with new X-LoRA config not yet implemented");
        tracing::info!("üí° Would reload {} with adapters {:?}", model_path, adapter_ids);
        
        // For now, just track the intended configuration
        self.adaptation_state.adaptation_mode = AdaptationMode::XLoRA {
            active_adapters: adapter_ids.to_vec(),
            max_adapters: adapter_ids.len(),
        };
        
        Ok(())
    }
    
    /// Extract real model metadata from mistral.rs
    async fn extract_model_metadata(&self, model: &mistralrs::Model) -> Result<ModelInfo> {
        tracing::debug!("üîç Extracting model metadata from mistral.rs");
        
        // Try to get model configuration
        let config = model.config()
            .map_err(|e| anyhow!("Failed to get model config: {}", e))?;
        
        // Try to get general metadata
        let inner = model.inner();
        // Note: get_metadata is not available, use defaults
        
        // Determine architecture
        let architecture = self.determine_architecture(&config);
        
        // Get context length (default for now)
        let context_length = 4096;
        
        // Get additional details (use defaults for now)
        let (hidden_size, _num_attn_heads, parameters) = (4096, 32, 7_000_000_000);
        let num_hidden_layers = 32; // Default value
        
        // Estimate vocab size based on model type
        let vocab_size = self.estimate_vocab_size(&architecture);
        
        // Get quantization info
        let quantization = self.get_quantization_info(&config);
        
        // Get model name
        let name = self.get_model_name();
        
        tracing::info!("üìä Extracted metadata: {} layers, {}D hidden, {}k context", 
                      num_hidden_layers, hidden_size, context_length / 1000);
        
        Ok(ModelInfo {
            name,
            parameters,
            context_length,
            vocab_size,
            architecture,
            quantization,
        })
    }
    
    /// Get fallback model info when metadata extraction fails
    fn get_fallback_model_info(&self, path: &Path, suffix: &str) -> ModelInfo {
        let name = format!("{}-{}", 
            path.file_stem()
                .and_then(|s| s.to_str())
                .unwrap_or("unknown"), 
            suffix
        );
        
        ModelInfo {
            name,
            parameters: 0, // Unknown
            context_length: self.config.context_length,
            vocab_size: 32000, // Common default
            architecture: format!("mistral.rs-{}", suffix),
            quantization: Some("Q8_0".to_string()), // Default
        }
    }
    
    /// Determine architecture from mistral.rs config
    fn determine_architecture(&self, config: &mistralrs::MistralRsConfig) -> String {
        use mistralrs::ModelKind;
        
        match &config.kind {
            ModelKind::Normal => "mistral.rs-normal".to_string(),
            ModelKind::GgufQuantized { .. } => "mistral.rs-gguf".to_string(),
            ModelKind::Adapter { adapter: _, .. } => "mistral.rs-adapter".to_string(),
            ModelKind::GgufAdapter { adapter: _, .. } => "mistral.rs-gguf-adapter".to_string(),
            ModelKind::Speculative { .. } => "mistral.rs-speculative".to_string(),
            ModelKind::AnyMoe { .. } => "mistral.rs-anymoe".to_string(),
        }
    }
    
    /// Estimate vocab size based on model architecture or path
    fn estimate_vocab_size(&self, architecture: &str) -> usize {
        match &self.builder_config {
            ModelBuilderConfig::Gguf { model_path, .. } |
            ModelBuilderConfig::Text { model_id: model_path, .. } |
            ModelBuilderConfig::Lora { base_model_path: model_path, .. } |
            ModelBuilderConfig::XLora { base_model_path: model_path, .. } => {
                let path_lower = model_path.to_lowercase();
                if path_lower.contains("qwen") {
                    151936
                } else if path_lower.contains("mistral") {
                    32000
                } else if path_lower.contains("llama") {
                    32000
                } else if path_lower.contains("phi") {
                    51200
                } else {
                    32000 // Default
                }
            }
        }
    }
    
    /// Estimate parameter count based on architecture
    fn estimate_parameters(&self, hidden_size: usize, num_layers: usize, architecture: &str) -> usize {
        if hidden_size == 0 || num_layers == 0 {
            return 0;
        }
        
        let vocab_size = self.estimate_vocab_size(architecture);
        
        // Rough parameter estimation for transformer models
        let embedding_params = vocab_size * hidden_size * 2; // input + output embeddings
        
        // Per-layer parameters (very rough approximation):
        // - Self-attention: 4 * hidden_size^2 (Q, K, V, O projections)
        // - Feed-forward: 8 * hidden_size^2 (typically 4x expansion then back down)
        // - Layer norm: 2 * hidden_size (small, negligible)
        let layer_params = num_layers * hidden_size * hidden_size * 12;
        
        embedding_params + layer_params
    }
    
    /// Get quantization information from config
    fn get_quantization_info(&self, config: &mistralrs::MistralRsConfig) -> Option<String> {
        use mistralrs::ModelKind;
        
        match &config.kind {
            ModelKind::GgufQuantized { quant: _ } => Some("quantized".to_string()),
            ModelKind::GgufAdapter { quant: _, .. } => Some("adapter".to_string()),
            _ => self.get_quantization_from_config().map(|q| format!("{:?}", q)),
        }
    }
    
    /// Get model name based on builder configuration
    fn get_model_name(&self) -> String {
        match &self.builder_config {
            ModelBuilderConfig::Gguf { model_path, .. } => {
                Path::new(model_path)
                    .file_stem()
                    .and_then(|s| s.to_str())
                    .unwrap_or("gguf-model")
                    .to_string()
            }
            ModelBuilderConfig::Text { model_id, .. } => model_id.clone(),
            ModelBuilderConfig::Lora { base_model_path, .. } => {
                format!("{}-lora", 
                    Path::new(base_model_path)
                        .file_stem()
                        .and_then(|s| s.to_str())
                        .unwrap_or("lora-model")
                )
            }
            ModelBuilderConfig::XLora { base_model_path, .. } => {
                format!("{}-xlora", 
                    Path::new(base_model_path)
                        .file_stem()
                        .and_then(|s| s.to_str())
                        .unwrap_or("xlora-model")
                )
            }
        }
    }
}

/// X-LoRA routing strategy
#[derive(Debug, Clone)]
pub enum XLoraRoutingStrategy {
    /// Learn optimal routing automatically
    Learned,
    /// Use manual routing configuration
    Manual(HashMap<String, f32>),
    /// Route based on performance metrics
    Performance,
}

// TODO: Implement conversion once mistralrs dependency is resolved
// impl Into<mistral_rs::XLoraRoutingStrategy> for XLoraRoutingStrategy {
//     fn into(self) -> mistral_rs::XLoraRoutingStrategy {
//         match self {
//             XLoraRoutingStrategy::Learned => mistral_rs::XLoraRoutingStrategy::Learned,
//             XLoraRoutingStrategy::Manual(routing) => mistral_rs::XLoraRoutingStrategy::Manual(routing),
//             XLoraRoutingStrategy::Performance => mistral_rs::XLoraRoutingStrategy::Performance,
//         }
//     }
// }

/// User feedback for adaptation
#[derive(Debug, Clone)]
pub struct UserFeedback {
    pub quality_score: f32,  // 0.0 to 1.0
    pub helpful: bool,
    pub corrections: Option<String>,
    pub timestamp: chrono::DateTime<chrono::Utc>,
}

// Extend GenerationRequest to support X-LoRA features
impl GenerationRequest {
    pub fn with_adapters(mut self, adapter_ids: Vec<String>) -> Self {
        self.active_adapters = Some(adapter_ids);
        self
    }
    
    pub fn requires_realtime_adaptation(&self) -> bool {
        self.realtime_adaptation.is_some()
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test]
    async fn test_engine_creation() {
        let engine = MistralEngine::new_default();
        assert!(engine.is_ok());
        
        let engine = engine.unwrap();
        assert!(!engine.is_loaded());
    }

    #[tokio::test]
    async fn test_xlora_configuration() {
        let mut engine = MistralEngine::new_default().unwrap();
        
        // Should fail when no model is loaded
        let result = engine.configure_xlora(4, XLoraRoutingStrategy::Learned).await;
        assert!(result.is_err());
    }

    #[test]
    fn test_adaptation_mode_default() {
        let state = AdaptationState::default();
        matches!(state.adaptation_mode, AdaptationMode::Disabled);
    }
}
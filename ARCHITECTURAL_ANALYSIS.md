# Hyprstream Architectural Analysis & Critical Fix Plan

## Executive Summary

**CRITICAL ISSUE DISCOVERED:** The inference module architecture is fundamentally broken. `src/inference/` contains FlightSQL transport code instead of LLaMA.cpp inference logic, preventing actual model inference despite having functional LLaMA.cpp integration in `src/runtime/`.

## Current Architecture Problems

### üî• Critical Issues

```
‚ùå CURRENT BROKEN ARCHITECTURE:

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ            CLIENT REQUEST                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         src/api/ (Mixed)                    ‚îÇ
‚îÇ  ‚Ä¢ REST endpoints                           ‚îÇ
‚îÇ  ‚Ä¢ Some FlightSQL                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ    src/inference/ (WRONG!)                  ‚îÇ
‚îÇ  ‚Ä¢ inference_service.rs = FlightSQL server ‚îÇ  ‚ùå
‚îÇ  ‚Ä¢ Arrow/transport code                     ‚îÇ  ‚ùå
‚îÇ  ‚Ä¢ NO actual inference logic                ‚îÇ  ‚ùå
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ    src/runtime/ (Good but unused)           ‚îÇ
‚îÇ  ‚Ä¢ llamacpp_engine.rs = LLaMA.cpp          ‚îÇ  ‚úÖ
‚îÇ  ‚Ä¢ Proper model loading                     ‚îÇ  ‚úÖ
‚îÇ  ‚Ä¢ NOT CONNECTED to inference layer        ‚îÇ  ‚ùå
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

RESULT: No actual model inference possible! üö´
```

### Files in Wrong Places

| File | Current Location | Should Be | Issue |
|------|------------------|-----------|-------|
| `inference_service.rs` | `src/inference/` | `src/api/` | FlightSQL is transport, not inference |
| LLaMA.cpp integration | `src/runtime/` | Used by `src/inference/` | Not properly connected |
| Core inference logic | **MISSING** | `src/inference/` | No actual ML inference |

## Target Architecture (Fixed)

```
‚úÖ CORRECT TARGET ARCHITECTURE:

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                CLIENT LAYER                 ‚îÇ
‚îÇ  CLI ‚Ä¢ REST ‚Ä¢ FlightSQL ‚Ä¢ Browser          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ          API TRANSPORT LAYER                ‚îÇ
‚îÇ              src/api/                       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ server.rs (REST endpoints)            ‚îÇ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ flight_service.rs (FlightSQL server)  ‚îÇ‚îÇ  ‚Üê MOVED HERE
‚îÇ  ‚îÇ ‚Ä¢ openai_compat.rs (OpenAI API)         ‚îÇ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ All transport protocols HERE          ‚îÇ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ .generate(prompt, params)
                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         INFERENCE ORCHESTRATION             ‚îÇ
‚îÇ             src/inference/                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ inference_engine.rs (REWRITE)         ‚îÇ‚îÇ  ‚Üê PURE ML LOGIC
‚îÇ  ‚îÇ   - session_management()                ‚îÇ‚îÇ
‚îÇ  ‚îÇ   - generate(prompt) -> text            ‚îÇ‚îÇ
‚îÇ  ‚îÇ   - load_model(path)                    ‚îÇ‚îÇ
‚îÇ  ‚îÇ   - apply_lora_adapters()               ‚îÇ‚îÇ
‚îÇ  ‚îÇ                                         ‚îÇ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ lora_fusion.rs (enhance)              ‚îÇ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ model_loader.rs (enhance)             ‚îÇ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ .generate_text(prompt)
                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ           RUNTIME EXECUTION                 ‚îÇ
‚îÇ             src/runtime/                    ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ llamacpp_engine.rs (ENHANCE)          ‚îÇ‚îÇ  ‚Üê ACTUAL LLaMA.cpp
‚îÇ  ‚îÇ   - LlamaBackend, LlamaModel            ‚îÇ‚îÇ
‚îÇ  ‚îÇ   - generate_text() ‚Üê ADD THIS          ‚îÇ‚îÇ
‚îÇ  ‚îÇ   - load_model() ‚úì exists               ‚îÇ‚îÇ
‚îÇ  ‚îÇ   - tokenize(), decode()                ‚îÇ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              STORAGE                        ‚îÇ
‚îÇ             src/storage/                    ‚îÇ
‚îÇ  ‚Ä¢ Model files (GGUF, SafeTensors)         ‚îÇ
‚îÇ  ‚Ä¢ LoRA adapters                           ‚îÇ
‚îÇ  ‚Ä¢ VDB storage (optional)                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## Critical Changes Required

### üî• URGENT (Next 24 hours)

#### 1. Move FlightSQL Service to API Layer
```bash
# Move transport code to correct location
git mv src/inference/inference_service.rs src/api/flight_service.rs

# Update module declarations
# - Remove from src/inference/mod.rs
# - Add to src/api/mod.rs
```

#### 2. Rewrite Inference Engine for Pure ML Logic
**File: `src/inference/inference_engine.rs`**

**Current Problems:**
```rust
// WRONG: Mixed transport concerns
use arrow_flight::FlightService;  // ‚ùå Transport in inference
use tonic::Request;               // ‚ùå gRPC in inference

// WRONG: Placeholder inference
async fn generate(&self, prompt: &str) -> Result<String> {
    Ok("placeholder response".to_string())  // ‚ùå Not using LLaMA.cpp!
}
```

**Target Implementation:**
```rust
// CORRECT: Pure ML logic
use crate::runtime::LlamaCppEngine;      // ‚úÖ Use actual LLaMA.cpp
use crate::adapters::LoRAAdapter;        // ‚úÖ LoRA integration

pub struct InferenceEngine {
    llama_engine: LlamaCppEngine,           // ‚úÖ Real LLaMA.cpp engine
    sessions: HashMap<String, Session>,    // ‚úÖ Session management
    lora_registry: LoRARegistry,           // ‚úÖ Adapter management
}

impl InferenceEngine {
    /// Generate text using LLaMA.cpp (NOT placeholder!)
    pub async fn generate(&self, prompt: &str, params: GenerationParams) -> Result<String> {
        // 1. Load model if not loaded
        // 2. Apply LoRA adapters if specified
        // 3. Call llama_engine.generate_text(prompt)
        // 4. Return actual generated text
        self.llama_engine.generate_text(prompt, params.max_tokens).await
    }
    
    /// Load GGUF model via LLaMA.cpp
    pub async fn load_model(&mut self, model_path: &Path) -> Result<()> {
        self.llama_engine.load_model(model_path).await
    }
}
```

#### 3. Enhance LlamaCppEngine with Missing Methods
**File: `src/runtime/llamacpp_engine.rs`**

**Current State:** Good foundation, missing key methods

**Add These Methods:**
```rust
impl LlamaCppEngine {
    /// Generate text using llama-cpp-2 (CRITICAL MISSING METHOD)
    pub async fn generate_text(&self, prompt: &str, max_tokens: usize) -> Result<String> {
        // 1. Ensure model is loaded
        let model = self.model.as_ref()
            .ok_or_else(|| anyhow!("No model loaded"))?;
            
        // 2. Create context if needed
        let context = self.create_context()?;
        
        // 3. Tokenize input
        let tokens = model.tokenize(prompt, true)?;
        
        // 4. Generate tokens auto-regressively
        let mut generated_tokens = Vec::new();
        for _ in 0..max_tokens {
            let logits = context.eval(&tokens)?;
            let next_token = self.sample_token(&logits)?;
            generated_tokens.push(next_token);
            
            // Check for EOS token
            if self.is_eos_token(next_token) {
                break;
            }
        }
        
        // 5. Decode to text
        let generated_text = model.detokenize(&generated_tokens)?;
        Ok(generated_text)
    }
    
    /// Check if model is loaded and ready
    pub fn is_ready(&self) -> bool {
        self.model.is_some() && self.backend.is_some()
    }
}
```

### üìã Data Flow (Fixed Architecture)

```
1. CLIENT REQUEST
   "Generate text: Hello world"
   ‚îÇ
   ‚ñº
2. API LAYER (src/api/)
   ‚Ä¢ Parse REST/FlightSQL request
   ‚Ä¢ Extract: prompt="Hello world", max_tokens=50
   ‚Ä¢ Call: inference_engine.generate(prompt, params)
   ‚îÇ
   ‚ñº
3. INFERENCE ENGINE (src/inference/)
   ‚Ä¢ Route to appropriate model session
   ‚Ä¢ Apply LoRA adapters if specified
   ‚Ä¢ Call: llama_engine.generate_text(prompt, 50)
   ‚îÇ
   ‚ñº
4. LLAMA.CPP ENGINE (src/runtime/)
   ‚Ä¢ tokenize("Hello world") -> [1, 2, 3]
   ‚Ä¢ Auto-regressive generation loop
   ‚Ä¢ Generate 50 tokens: [4, 5, 6, ...]
   ‚Ä¢ detokenize([4, 5, 6, ...]) -> "Hello world, how are you today?"
   ‚îÇ
   ‚ñº
5. RESPONSE
   ‚Ä¢ Return generated text up the stack
   ‚Ä¢ Format as JSON/FlightSQL/text as needed
   ‚Ä¢ Stream to client
```

## Implementation Steps (Priority Order)

### Phase 1: Architecture Fix (Day 1)

#### Step 1.1: Move FlightSQL Service (2 hours)
```bash
# Move the file
mv src/inference/inference_service.rs src/api/flight_service.rs

# Update src/api/mod.rs
pub mod flight_service;
pub use flight_service::*;

# Update src/inference/mod.rs  
// Remove: pub mod inference_service;
```

#### Step 1.2: Rewrite Inference Engine (4 hours)
- Remove all FlightSQL/Arrow imports
- Add LlamaCppEngine integration  
- Implement actual generate() method
- Add session management

#### Step 1.3: Enhance LlamaCppEngine (2 hours)
- Add generate_text() method
- Add proper tokenization flow
- Add error handling for model loading

### Phase 2: Integration Testing (Day 2)

#### Test Commands:
```bash
# Test 1: Basic compilation
cargo build --no-default-features

# Test 2: Model loading
./target/debug/hyprstream model load path/to/model.gguf

# Test 3: Actual inference (KEY TEST)
./target/debug/hyprstream chat "Hello, tell me about yourself"
# Expected: Real LLaMA.cpp generated response, NOT placeholder

# Test 4: API endpoint
curl -X POST localhost:8080/v1/chat/completions \
  -d '{"messages":[{"role":"user","content":"Hello"}]}'
# Expected: Real generated JSON response
```

### Success Criteria

#### ‚úÖ Phase 1 Complete When:
- [ ] `src/inference/` contains NO FlightSQL/Arrow code
- [ ] `src/api/` contains ALL transport protocols
- [ ] `inference_engine.rs` uses `LlamaCppEngine` directly
- [ ] `cargo build` succeeds with clean architecture

#### ‚úÖ Phase 2 Complete When:
- [ ] `hyprstream chat "test"` generates real LLaMA.cpp response
- [ ] No "placeholder" or mock text in output
- [ ] Model loading works end-to-end
- [ ] API endpoints return actual inference results

## Files Requiring Changes

### üî• REWRITE REQUIRED

| File | Action | Priority | Effort |
|------|--------|----------|---------|
| `src/inference/inference_engine.rs` | Complete rewrite | URGENT | 4h |
| `src/inference/mod.rs` | Remove FlightSQL exports | URGENT | 1h |
| `src/api/mod.rs` | Add FlightSQL service | URGENT | 1h |

### üìÅ MOVE REQUIRED

| File | From | To | Priority | Effort |
|------|------|----|----------|---------|
| `inference_service.rs` | `src/inference/` | `src/api/flight_service.rs` | URGENT | 2h |

### üîß ENHANCE REQUIRED

| File | Changes | Priority | Effort |
|------|---------|----------|---------|
| `src/runtime/llamacpp_engine.rs` | Add `generate_text()` method | URGENT | 2h |
| `src/cli/handlers.rs` | Use new inference API | MEDIUM | 1h |

### ‚úÖ GOOD SHAPE (No changes needed)

| File | Status | Notes |
|------|--------|-------|
| `src/storage/` modules | ‚úÖ Good | Recently fixed, proper architecture |
| `src/adapters/` modules | ‚úÖ Good | Basic structure exists |
| `src/models/` modules | ‚úÖ Good | Qwen3 integration foundation |

## Risk Assessment

### üî¥ High Risk
1. **LlamaCpp Integration Complexity**
   - **Mitigation:** Start with basic text generation, add features incrementally
   - **Fallback:** Ensure error messages are clear if integration fails

### üü° Medium Risk  
1. **API Compatibility After Move**
   - **Mitigation:** Test all endpoints after FlightSQL service move
   - **Plan:** Gradual testing of REST, CLI, and FlightSQL interfaces

### üü¢ Low Risk
1. **VDB Storage Integration**  
   - **Status:** Already conditional with feature flags ‚úÖ
   - **Plan:** Can be developed independently

## Success Validation

### Architecture Validation
```bash
# Ensure no transport code in inference
find src/inference -name "*.rs" -exec grep -l "FlightSQL\|Arrow\|tonic" {} \;
# Expected: Empty result (0 files found)

# Ensure FlightSQL is in API layer  
find src/api -name "*.rs" -exec grep -l "FlightSQL" {} \;
# Expected: At least 1 file (the moved service)
```

### Functional Validation  
```bash
# Test actual inference (most important test)
echo "What is 2+2?" | ./target/debug/hyprstream chat
# Expected: Real mathematical response from LLaMA.cpp
# NOT: "placeholder response" or mock text
```

This architectural fix plan provides a direct path from the current broken state to functional LLaMA.cpp inference with proper separation of concerns.
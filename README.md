# Hyprstream: VDB-First Adaptive ML Inference Engine ğŸš€

[![Rust](https://github.com/hyprstream/hyprstream/actions/workflows/rust.yml/badge.svg)](https://github.com/hyprstream/hyprstream/actions/workflows/rust.yml)

âš ï¸ **ALPHA RELEASE**: This project is undergoing a major architectural shift from data processing to ML inference with VDB storage. Active development in progress. âš ï¸

## ğŸŒŸ Overview

Hyprstream is pioneering a new approach to ML inference through **99% sparse neural networks** with **VDB (OpenVDB) storage** and **temporal LoRA adaptation**. By storing model weights in hierarchical sparse formats and enabling real-time weight updates during inference, Hyprstream achieves unprecedented efficiency and adaptability.

### ğŸ¯ Core Innovation

Traditional ML inference engines load dense models into memory. Hyprstream takes a radically different approach:
- **Sparse by Default**: 99% of weights are zero and never loaded
- **VDB Storage**: Hierarchical sparse grids for efficient weight access  
- **Temporal Adaptation**: Weights evolve during inference based on context
- **Zero-Copy Operations**: Memory-mapped weights with on-demand loading

## âœ¨ Key Features

### ğŸ§  Sparse Neural Networks
- **99% Sparsity**: Extreme weight pruning without accuracy loss
- **Structured Sparsity**: Prune entire channels/attention heads
- **Dynamic Masks**: Adjust sparsity patterns during inference

### ğŸ’¾ VDB-First Storage
- **OpenVDB Format**: Industry-standard volumetric data structure
- **Hierarchical Grids**: Multi-level sparse representation
- **Neural Compression**: 10-100x compression with custom codec
- **Hardware Acceleration**: GPU-accelerated sparse operations

### ğŸ”„ Temporal LoRA
- **Real-Time Updates**: Adapt weights during generation
- **Gradient Streaming**: Continuous learning from context
- **Checkpoint System**: Save/restore adapted model states
- **Multi-Adapter**: X-LoRA routing between specialized adapters

### âš¡ Performance
- **Memory Efficient**: 10x reduction in memory usage
- **Fast Inference**: Sparse operations skip zero weights
- **Disk-Backed**: Unlimited model size with mmap
- **Batched Updates**: Efficient sparse weight modifications

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             CLI Interface                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚          Candle Runtime Engine              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   GGUF   â”‚â†’ â”‚  Sparse  â”‚â†’ â”‚   VDB    â”‚ â”‚
â”‚  â”‚  Loader  â”‚  â”‚Conversionâ”‚  â”‚ Storage  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚      Temporal Streaming Layer               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Gradient â”‚â†’ â”‚  Weight  â”‚â†’ â”‚Checkpointâ”‚ â”‚
â”‚  â”‚  Stream  â”‚  â”‚  Update  â”‚  â”‚  System  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚          OpenVDB Integration                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  C++ FFI â”‚  â”‚  Sparse  â”‚  â”‚  Memory  â”‚ â”‚
â”‚  â”‚  Bridge  â”‚  â”‚   Grids  â”‚  â”‚   Maps   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites
- Rust 1.75+
- OpenVDB 9.0+ (system package)
- CUDA 12.0+ (optional, for GPU)

### Installation

```bash
# Clone repository
git clone https://github.com/hyprstream/hyprstream.git
cd hyprstream

# Build with OpenVDB support
cargo build --release

# Verify OpenVDB integration
cargo run -- --version
```

### Basic Usage

```bash
# Start server
hyprstream server --port 50051

# Download model from HuggingFace
hyprstream model download hf://mistralai/Mistral-7B-v0.1

# Interactive chat
hyprstream chat --model /path/to/model.gguf

# Chat with real-time training
hyprstream chat --model /path/to/model.gguf --train
```

## ğŸ”§ Configuration

### Storage Configuration
```toml
[storage]
path = "./vdb_storage"
neural_compression = true      # Enable 10-100x compression
hardware_acceleration = true   # Use GPU if available
cache_size_mb = 2048           # Hot adapter cache
compaction_interval_secs = 300 # Background optimization
```

### Runtime Configuration
```toml
[runtime]
use_gpu = true
cpu_threads = 8
context_length = 4096
batch_size = 1
```

### Generation Configuration
```toml
[generation]
max_tokens = 2048
temperature = 0.7
top_p = 0.9
frequency_penalty = 0.0
presence_penalty = 0.0
```

## ğŸ§ª Current Status

### âœ… Implemented
- Candle ML framework integration
- OpenVDB C++ bindings
- VDB sparse storage layer
- GGUF model loading
- Tensor sparsification
- CLI interface
- Basic chat functionality

### ğŸš§ In Progress
- Temporal gradient streaming
- Real-time weight updates
- X-LoRA multi-adapter routing
- Tokenizer integration
- Generation with VDB weights

### ğŸ“‹ Planned
- Distributed inference
- Advanced quantization (Q4, Q8)
- Web UI dashboard
- Model fine-tuning
- ONNX support

## ğŸ”¬ Technical Deep Dive

### Sparsification Process
1. **Load GGUF**: Read quantized model weights
2. **Identify Sparse Tensors**: Select attention/FFN layers
3. **Apply Pruning**: Remove weights below threshold
4. **Create VDB Grid**: Store in hierarchical format
5. **Build Index**: Create spatial acceleration structure

### VDB Grid Structure
```
Root (Level 3) â†’ 8Â³ branches
  â”œâ”€ Internal (Level 2) â†’ 4Â³ voxels  
  â”‚   â”œâ”€ Internal (Level 1) â†’ 2Â³ voxels
  â”‚   â”‚   â””â”€ Leaf (Level 0) â†’ Active values
  â”‚   â””â”€ Tile â†’ Constant region
  â””â”€ Background â†’ Zero/pruned weights
```

### Temporal Adaptation Flow
```
Input â†’ Tokenize â†’ Forward Pass â†’ Generate
           â†“            â†“            â†“
      [Gradients] â† [Weights] â† [Updates]
           â†“            â†“            â†“
      [Checkpoint] â†’ [VDB Store] â†’ [Persist]
```

## ğŸ¤ Contributing

We welcome contributions! Key areas:
- Performance optimizations
- Model format support
- Quantization methods
- Documentation
- Testing

See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

## ğŸ“š References

### Papers
- [Candle: Minimalist ML Framework](https://github.com/huggingface/candle)
- [OpenVDB: Efficient Sparse Volumes](https://www.openvdb.org/documentation/)
- [LoRA: Low-Rank Adaptation](https://arxiv.org/abs/2106.09685)

### Related Projects
- [vLLM](https://github.com/vllm-project/vllm) - High-throughput inference
- [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM) - NVIDIA optimization
- [llama.cpp](https://github.com/ggerganov/llama.cpp) - CPU inference

## ğŸ“„ License

GNU AFFERO GENERAL PUBLIC LICENSE 3

## ğŸ™ Acknowledgments

Built with:
- [Candle](https://github.com/huggingface/candle) - Rust ML framework
- [OpenVDB](https://www.openvdb.org/) - Sparse volumetric data
- [Tokio](https://tokio.rs/) - Async runtime
- [Tonic](https://github.com/hyperium/tonic) - gRPC framework

---

**Note**: This project has pivoted from its original data processing focus to become a specialized ML inference engine. The old Arrow Flight SQL functionality has been deprecated in favor of VDB-based model serving.

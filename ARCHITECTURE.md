# Hyprstream Architecture Diagram

## Executive Summary
This document outlines the target architecture for Hyprstream, identifying critical issues with the current implementation and providing a clear path to functional LLaMA.cpp inference integration.

## Current State Analysis

### âŒ Critical Issues Identified
1. **Mixed Concerns**: `src/inference/` contains FlightSQL transport code instead of ML logic
2. **Unused LLaMA.cpp**: Proper LLaMA.cpp integration exists in `src/runtime/` but isn't properly utilized
3. **API Confusion**: Transport protocols mixed with core inference logic
4. **Missing Integration**: No clear path from API requests to actual model inference

## Target Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          CLIENT LAYER                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  CLI Commands     â”‚  REST Clients    â”‚  FlightSQL Clients       â”‚
â”‚  hyprstream chat  â”‚  curl/browser    â”‚  DBeaver/BI tools        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        API TRANSPORT LAYER                      â”‚
â”‚                         src/api/                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚   REST API      â”‚ â”‚  FlightSQL      â”‚ â”‚    CLI Handlers     â”‚ â”‚
â”‚ â”‚   (Axum)        â”‚ â”‚  Service        â”‚ â”‚   (Clap)            â”‚ â”‚
â”‚ â”‚                 â”‚ â”‚                 â”‚ â”‚                     â”‚ â”‚
â”‚ â”‚ â€¢ Chat endpoint â”‚ â”‚ â€¢ SQL interface â”‚ â”‚ â€¢ model download    â”‚ â”‚
â”‚ â”‚ â€¢ OpenAI compat â”‚ â”‚ â€¢ Arrow data    â”‚ â”‚ â€¢ chat command      â”‚ â”‚
â”‚ â”‚ â€¢ Model mgmt    â”‚ â”‚ â€¢ Streaming     â”‚ â”‚ â€¢ server start      â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    INFERENCE ORCHESTRATION                      â”‚
â”‚                        src/inference/                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚                 InferenceEngine                             â”‚ â”‚
â”‚ â”‚                                                             â”‚ â”‚
â”‚ â”‚  â€¢ session_management()                                     â”‚ â”‚
â”‚ â”‚  â€¢ generate(prompt, params) -> InferenceResult             â”‚ â”‚
â”‚ â”‚  â€¢ load_model(path) -> ModelHandle                          â”‚ â”‚
â”‚ â”‚  â€¢ apply_lora_adapters(adapters)                            â”‚ â”‚
â”‚ â”‚  â€¢ stream_tokens() -> TokenStream                           â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                â”‚                                â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚              LoRA Fusion Engine                             â”‚ â”‚
â”‚ â”‚                                                             â”‚ â”‚
â”‚ â”‚  â€¢ merge_adapters(base, adapters) -> FusedWeights          â”‚ â”‚
â”‚ â”‚  â€¢ dynamic_routing(input) -> AdapterSelection               â”‚ â”‚
â”‚ â”‚  â€¢ sparse_application(weights) -> OptimizedModel           â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       RUNTIME EXECUTION                         â”‚
â”‚                        src/runtime/                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚                 LlamaCppEngine                              â”‚ â”‚
â”‚ â”‚                    [KEEP - GOOD]                            â”‚ â”‚
â”‚ â”‚                                                             â”‚ â”‚
â”‚ â”‚  â€¢ llama_backend: LlamaBackend                              â”‚ â”‚
â”‚ â”‚  â€¢ llama_model: LlamaModel                                  â”‚ â”‚
â”‚ â”‚  â€¢ llama_context: LlamaContext                              â”‚ â”‚
â”‚ â”‚                                                             â”‚ â”‚
â”‚ â”‚  Methods:                                                   â”‚ â”‚
â”‚ â”‚  â€¢ load_model(path) -> Result<()>                           â”‚ â”‚
â”‚ â”‚  â€¢ tokenize(text) -> Vec<TokenId>                           â”‚ â”‚
â”‚ â”‚  â€¢ generate_tokens(prompt) -> TokenStream                   â”‚ â”‚
â”‚ â”‚  â€¢ apply_weights(lora_weights) -> Result<()>                â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       STORAGE BACKENDS                          â”‚
â”‚                        src/storage/                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚  Model Store  â”‚ â”‚  VDB Storage  â”‚ â”‚    LoRA Registry        â”‚ â”‚
â”‚ â”‚               â”‚ â”‚   [OPTIONAL]  â”‚ â”‚                         â”‚ â”‚
â”‚ â”‚ â€¢ GGUF files  â”‚ â”‚               â”‚ â”‚ â€¢ Adapter metadata      â”‚ â”‚
â”‚ â”‚ â€¢ SafeTensors â”‚ â”‚ â€¢ OpenVDB     â”‚ â”‚ â€¢ Sparse weights        â”‚ â”‚
â”‚ â”‚ â€¢ HF Download â”‚ â”‚ â€¢ Neural comp â”‚ â”‚ â€¢ Training history      â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Data Flow for Inference Request

```
1. CLIENT REQUEST
   â”‚
   â–¼
2. API LAYER (src/api/)
   â”‚ Parse request (REST/FlightSQL/CLI)
   â”‚ Validate parameters
   â”‚ Extract session info
   â–¼
3. INFERENCE ORCHESTRATION (src/inference/)
   â”‚ Route to appropriate model
   â”‚ Load LoRA adapters if specified
   â”‚ Prepare generation parameters
   â–¼
4. LORA FUSION (src/inference/)
   â”‚ Merge base model + LoRA weights
   â”‚ Apply sparse optimizations
   â”‚ Create fused model state
   â–¼
5. RUNTIME EXECUTION (src/runtime/)
   â”‚ LlamaCppEngine.generate_tokens()
   â”‚ Apply tokenization
   â”‚ Run inference loop
   â”‚ Stream tokens back
   â–¼
6. RESPONSE STREAMING
   â”‚ Format tokens (text/JSON/Arrow)
   â”‚ Apply post-processing
   â”‚ Return to client
```

## Required Changes by Module

### ðŸ”¥ URGENT FIXES NEEDED

#### src/inference/ - Complete Rewrite Required
**Current Problems:**
- `inference_service.rs` contains FlightSQL server code âŒ
- Mixed transport concerns with ML logic âŒ
- No actual LLaMA.cpp integration âŒ

**Target Structure:**
```
src/inference/
â”œâ”€â”€ mod.rs                    [REWRITE] - Clean module exports
â”œâ”€â”€ inference_engine.rs       [REWRITE] - Core ML orchestration
â”œâ”€â”€ lora_fusion.rs           [KEEP/ENHANCE] - LoRA weight merging
â”œâ”€â”€ model_loader.rs          [ENHANCE] - Model loading utilities
â”œâ”€â”€ session_manager.rs       [NEW] - Session state management
â””â”€â”€ token_processor.rs       [NEW] - Tokenization utilities
```

**Key Methods Needed:**
```rust
impl InferenceEngine {
    async fn generate(&self, session_id: &str, prompt: &str, params: GenerationParams) -> Result<TokenStream>;
    async fn load_model(&mut self, model_path: &Path) -> Result<ModelHandle>;
    async fn create_session(&self, model_id: &str, lora_adapters: Vec<String>) -> Result<String>;
    async fn apply_lora_weights(&mut self, session_id: &str, adapters: &[LoRAAdapter]) -> Result<()>;
}
```

#### src/api/ - Move FlightSQL Here
**Required Actions:**
- Move `src/inference/inference_service.rs` â†’ `src/api/flight_service.rs` âœ…
- Keep all transport protocol code in API layer âœ…
- API calls inference engine, not the other way around âœ…

#### src/runtime/ - Good Foundation, Needs Enhancement
**Current State:** âœ… Good LLaMA.cpp integration exists
**Enhancements Needed:**
- Better error handling for model loading
- Support for LoRA weight application
- Context management improvements
- Memory optimization

### ðŸŸ¡ MODERATE FIXES NEEDED

#### src/storage/ - Partial Cleanup
**VDB Features:**
- Keep VDB storage for advanced use cases âœ…
- Make it optional and non-blocking âœ…
- Fix any remaining compilation issues âœ…

**Model Storage:**
- Enhance HuggingFace integration âœ…
- Better model caching âœ…
- GGUF format validation âœ…

#### src/adapters/ - Enhancement
**Current State:** Basic structure exists âœ…
**Needs:**
- Better LoRA loading from disk
- Runtime weight merging
- Memory-efficient sparse operations

### âœ… MODULES IN GOOD SHAPE

#### src/cli/ - Working Well
- Command structure is solid âœ…
- Good integration with storage âœ…
- Minor cleanup needed only âœ…

#### src/models/ - Qwen3 Integration
- Good foundation for model-specific code âœ…
- Can be extended for other model families âœ…

## Implementation Priority

### Phase 1: Critical Path to Working Inference (Week 1)
1. **Rewrite `src/inference/inference_engine.rs`**
   - Create clean interface that uses `LlamaCppEngine`
   - Remove all FlightSQL/transport code
   - Focus on pure ML logic

2. **Move FlightSQL service to API layer**
   - `src/inference/inference_service.rs` â†’ `src/api/flight_service.rs`
   - Update imports and dependencies
   - Ensure API calls inference engine

3. **Test basic inference pipeline**
   - Load GGUF model via LlamaCppEngine
   - Generate simple text responses
   - Verify end-to-end flow works

### Phase 2: LoRA Integration (Week 2)
1. **Enhance LoRA fusion engine**
   - Implement weight merging algorithms
   - Support multiple adapter loading
   - Memory-efficient operations

2. **Integrate with VDB storage**
   - Load LoRA weights from VDB
   - Cache merged weights
   - Optimize for real-time updates

### Phase 3: Production Ready (Week 3)
1. **Performance optimization**
   - Streaming responses
   - Context caching
   - Memory management

2. **API completeness**
   - OpenAI compatibility
   - Full FlightSQL support
   - Error handling

## Success Criteria

### âœ… Phase 1 Complete When:
- `hyprstream chat "Hello"` generates real LLaMA.cpp responses
- No FlightSQL code in `src/inference/`
- Clean separation between API and inference logic
- Basic model loading works end-to-end

### âœ… Phase 2 Complete When:
- LoRA adapters can be loaded and applied
- Multiple models can run simultaneously
- VDB storage integrates smoothly

### âœ… Phase 3 Complete When:
- Production-ready performance
- Full API compatibility
- Comprehensive error handling
- Documentation complete

## Key Architectural Principles

1. **Separation of Concerns**
   - API layer handles transport (REST, FlightSQL, CLI)
   - Inference layer handles ML logic only
   - Runtime layer handles model execution

2. **Clean Interfaces**
   - Each layer has well-defined public APIs
   - No leaking of internal implementation details
   - Easy to test and mock

3. **Scalability**
   - Support for multiple concurrent sessions
   - Efficient memory usage
   - Optional advanced features (VDB)

4. **Maintainability**
   - Clear module boundaries
   - Comprehensive error handling
   - Good documentation and testing

This architecture provides a clear path to functional LLaMA.cpp inference while maintaining the advanced VDB features that make Hyprstream unique.